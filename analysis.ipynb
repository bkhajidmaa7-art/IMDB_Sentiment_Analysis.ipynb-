{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnf3Y7Mtle4QQ2kRPL3i0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkhajidmaa7-art/IMDB_Sentiment_Analysis.ipynb-/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TAZcvO0nryTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343074bb-e1af-4563-ca9c-0d6f18549f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-14 18:57:01--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.2’\n",
            "\n",
            "aclImdb_v1.tar.gz.2 100%[===================>]  80.23M  45.7MB/s    in 1.8s    \n",
            "\n",
            "2025-12-14 18:57:03 (45.7 MB/s) - ‘aclImdb_v1.tar.gz.2’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim transformers nltk tqdm scikit-learn\n"
      ],
      "metadata": {
        "id": "grTfnwTJxzn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d7843c-0768-420f-f198-41941532af82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, numpy as np, nltk, torch\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "OiGRNpVdyFn_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n"
      ],
      "metadata": {
        "id": "-nB5MBr1zj3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be4bfaa9-b2ef-45cf-accb-98af7c64f1cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_imdb(path):\n",
        "    texts, labels = [], []\n",
        "    for label_type in [\"pos\", \"neg\"]:\n",
        "        folder = os.path.join(path, label_type)\n",
        "        label = 1 if label_type == \"pos\" else 0\n",
        "        for file in os.listdir(folder):\n",
        "            with open(os.path.join(folder, file), encoding=\"utf-8\") as f:\n",
        "                texts.append(f.read())\n",
        "                labels.append(label)\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "8MjeBY9n8v0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, train_labels = load_imdb(\"aclImdb/train\")\n",
        "test_texts, test_labels   = load_imdb(\"aclImdb/test\")\n"
      ],
      "metadata": {
        "id": "kTDM5HrHA_Or"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    tokens = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "6cp13cQWLPVA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_clean = [preprocess(t) for t in tqdm(train_texts)]\n",
        "test_clean  = [preprocess(t) for t in tqdm(test_texts)]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoG36wNvSzHL",
        "outputId": "1fa46042-3ca0-4bb2-c419-a0a1e5e20594"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [00:01<00:00, 16053.01it/s]\n",
            "100%|██████████| 25000/25000 [00:02<00:00, 8924.83it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(train_clean)\n",
        "X_test_tfidf  = tfidf.transform(test_clean)\n"
      ],
      "metadata": {
        "id": "1969u8j4S3dU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "tokenized = [t.split() for t in train_clean]\n",
        "\n",
        "w2v = Word2Vec(\n",
        "    sentences=tokenized,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "wkyPiq03TDoA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_vector(sentence):\n",
        "    words = sentence.split()\n",
        "    vecs = [w2v.wv[w] for w in words if w in w2v.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(100)\n"
      ],
      "metadata": {
        "id": "dfiyp1JaTYUn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v = np.array([sentence_vector(s) for s in train_clean])\n",
        "X_test_w2v  = np.array([sentence_vector(s) for s in test_clean])\n"
      ],
      "metadata": {
        "id": "DluAcOMgTa32"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "bert.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTYxTaC8TmSH",
        "outputId": "841a5867-de46-42bc-b555-b4ff02017506"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_tokenizer(tokenizer, data, max_length=128):\n",
        "    return tokenizer(\n",
        "        data,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "eNAiB2wgTyMX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_embed(device, tokenizer, bert, column, batch_size=8):\n",
        "    loader = DataLoader(column, batch_size=batch_size, shuffle=False)\n",
        "    outputs = []\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"BERT\"):\n",
        "        tokens = bert_tokenizer(tokenizer, batch)\n",
        "        tokens = {k:v.to(device) for k,v in tokens.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = bert(**tokens)\n",
        "\n",
        "        cls = out.last_hidden_state[:,0,:]\n",
        "        outputs.append(cls.cpu())\n",
        "\n",
        "    return torch.cat(outputs, dim=0)\n"
      ],
      "metadata": {
        "id": "uVNFKEqUT2Ac"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2500 positive + 2500 negative sample\n",
        "train_pos_idx = [i for i, y in enumerate(train_labels) if y == 1][:2500]\n",
        "train_neg_idx = [i for i, y in enumerate(train_labels) if y == 0][:2500]\n",
        "train_idx = train_pos_idx + train_neg_idx\n",
        "\n",
        "X_train_texts = [train_texts[i] for i in train_idx]\n",
        "y_train_bert   = [train_labels[i] for i in train_idx]\n",
        "\n",
        "# 1000 positive + 1000 negative sample\n",
        "test_pos_idx = [i for i, y in enumerate(test_labels) if y == 1][:1000]\n",
        "test_neg_idx = [i for i, y in enumerate(test_labels) if y == 0][:1000]\n",
        "test_idx = test_pos_idx + test_neg_idx\n",
        "\n",
        "X_test_texts = [test_texts[i] for i in test_idx]\n",
        "y_test_bert   = [test_labels[i] for i in test_idx]\n"
      ],
      "metadata": {
        "id": "xGaViKr2T44r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ],
      "metadata": {
        "id": "ajSBYP6wUo-P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic\": LogisticRegression(max_iter=1000),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=100)\n",
        "}\n"
      ],
      "metadata": {
        "id": "pBMEGN7DUr24"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bert = word_embed(device, tokenizer, bert, X_train_texts, batch_size=8)\n",
        "X_test_bert  = word_embed(device, tokenizer, bert, X_test_texts, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikn6IQ84XuJO",
        "outputId": "7dff033f-1e9c-4b07-c610-562be8d28fd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BERT: 100%|██████████| 625/625 [00:54<00:00, 11.47it/s]\n",
            "BERT: 100%|██████████| 250/250 [00:19<00:00, 12.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "\n",
        "def evaluate(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "        \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
        "        print(f\"{name}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "y73N7o4uYH-S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(X_train_bert, X_test_bert, y_train_bert, y_test_bert)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UXj-VejUx8B",
        "outputId": "0ac2961b-957e-46e2-b7d9-de0b686e8ab6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression: Acc=0.7995, F1=0.7947\n",
            "RandomForest: Acc=0.7525, F1=0.7524\n",
            "AdaBoost: Acc=0.7570, F1=0.7515\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'LogisticRegression': {'Accuracy': 0.7995, 'F1': 0.7946748591909882},\n",
              " 'RandomForest': {'Accuracy': 0.7525, 'F1': 0.752376188094047},\n",
              " 'AdaBoost': {'Accuracy': 0.757, 'F1': 0.7515337423312883}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}